{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Sample\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "data_path = spark.read.load('abfss://hpi@ag83stdev00006.dfs.core.windows.net/Campaign_Analytics/hpi.Campaign_Analytics.csv'\n",
        ", format='csv'\n",
        ", header=True\n",
        ")\n",
        "display(data_path.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load into Pandas and Perform Cleansing Operations\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "pd_df = data_path.select(\"*\").toPandas()\n",
        "\n",
        "# 1. Remove '$' symbol and convert datatype to float\n",
        "pd_df['Revenue']= pd_df['Revenue'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "pd_df['Revenue_Target']= pd_df['Revenue_Target'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# 2. Replace null values with 0\n",
        "pd_df['Revenue'].fillna(value=0, inplace=True)\n",
        "pd_df['Revenue_Target'].fillna(value=0, inplace=True)\n",
        "\n",
        "# 3. Convert columns to Camel Case\n",
        "pd_df['Region'] = pd_df.Region.str.title()\n",
        "pd_df['Country'] = pd_df.Country.str.title()\n",
        "pd_df['Campaign_Name'] = pd_df.Campaign_Name.str.title()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Transformation - Calculate Revenue Variance\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Create new column\n",
        "pd_df['Revenue_Variance'] = pd_df['Revenue_Target'] - pd_df['Revenue']\n",
        "\n",
        "print(pd_df[1:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Move data to Azure Data Lake Gen2\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import math\r\n",
        "import statistics\r\n",
        "import numpy as np\r\n",
        "import scipy.stats\r\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\n",
        "df = spark.createDataFrame(pd_df)\n",
        "df.show(5)\n",
        "\n",
        "(df\n",
        ".coalesce(1)\n",
        ".write.mode(\"overwrite\")\n",
        ".option(\"header\", \"true\")\n",
        ".format(\"com.databricks.spark.csv\")\n",
        ".save('abfss://hpi@ag83stdev00006.dfs.core.windows.net/Campaign_Analytics_Result/hpi.Campaign_Analytics.csv')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Retrieve Result from Azure Data Lake Gen2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "df = spark.read.load('abfss://hpi@ag83stdev00006.dfs.core.windows.net/Campaign_Analytics_Result/hpi.Campaign_Analytics.csv', format='csv'\r\n",
        ", header=True\r\n",
        ")\r\n",
        "display(df.limit(10))"
      ]
    }
  ]
}